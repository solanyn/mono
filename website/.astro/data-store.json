[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.1","content-config-digest","b7ba9b4abc414fa1","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://blog.goyangi.io\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":\"0.0.0.0\",\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,45,46,64,65],"k8s-at-home",{"id":11,"data":13,"body":19,"filePath":20,"digest":21,"rendered":22},{"title":14,"date":15,"tags":16},"Kubernetes at Home",["Date","2025-02-28T00:00:00.000Z"],[17,18],"kubernetes","home-ops","I've been managing a bare-metal Kubernetes cluster at home for a while now to learn about Kubernetes and its ecosystem. You can check it out here: [solanyn/home-ops](https://github.com/solanyn/home-ops)\n\n## The Cluster\n\nMy Kubernetes cluster is deployed with [Talos](https://talos.dev/) and is managed by [Flux](https://fluxcd.io/) via GitOps. I handle backups of the cluster to my NAS via NFS and S3 and offsite to Cloudflare R2.\n\n## Core Components\n\n- Flux: GitOps operator that reconciles cluster state from Git\n- Renovate: Automatically updates dependencies and creates PRs via GitHub Actions\n- cert-manager: Manages SSL certificates across services\n- external-dns: Syncs DNS records for ingresses, services and Gateway API HTTP routes\n- external-secrets: Integrates Kubernetes Secrets with 1Password\n- cilium: High-performance networking for Kubernetes\n- rook: Provides persistent storage for the cluster\n- volsync: Provides volume replication and dissaster recovery\n\n## DNS with Split Horizon\n\nI run two instances of `external-dns`:\n\n- One syncs private DNS records to my UniFi router using a webhook provider\n- The other pushes public records to Cloudflare\n\nIngresses/HTTPRoutes are tagged with `internal` or `external` classes to control which DNS provider is used.\n\n## Final Comments\n\nThis setup has been a great learning experience and has allowed me to explore the Kubernetes ecosystem in depth. I plan on building and expanding my cluster with compute, deploying more complex platforms and services (looking at you Kubeflow) and building AI/ML workloads on Kubernetes!","src/content/blog/k8s-at-home.md","d25610118d882ede",{"html":23,"metadata":24},"\u003Cp>I’ve been managing a bare-metal Kubernetes cluster at home for a while now to learn about Kubernetes and its ecosystem. You can check it out here: \u003Ca href=\"https://github.com/solanyn/home-ops\">solanyn/home-ops\u003C/a>\u003C/p>\n\u003Ch2 id=\"the-cluster\">The Cluster\u003C/h2>\n\u003Cp>My Kubernetes cluster is deployed with \u003Ca href=\"https://talos.dev/\">Talos\u003C/a> and is managed by \u003Ca href=\"https://fluxcd.io/\">Flux\u003C/a> via GitOps. I handle backups of the cluster to my NAS via NFS and S3 and offsite to Cloudflare R2.\u003C/p>\n\u003Ch2 id=\"core-components\">Core Components\u003C/h2>\n\u003Cul>\n\u003Cli>Flux: GitOps operator that reconciles cluster state from Git\u003C/li>\n\u003Cli>Renovate: Automatically updates dependencies and creates PRs via GitHub Actions\u003C/li>\n\u003Cli>cert-manager: Manages SSL certificates across services\u003C/li>\n\u003Cli>external-dns: Syncs DNS records for ingresses, services and Gateway API HTTP routes\u003C/li>\n\u003Cli>external-secrets: Integrates Kubernetes Secrets with 1Password\u003C/li>\n\u003Cli>cilium: High-performance networking for Kubernetes\u003C/li>\n\u003Cli>rook: Provides persistent storage for the cluster\u003C/li>\n\u003Cli>volsync: Provides volume replication and dissaster recovery\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"dns-with-split-horizon\">DNS with Split Horizon\u003C/h2>\n\u003Cp>I run two instances of \u003Ccode>external-dns\u003C/code>:\u003C/p>\n\u003Cul>\n\u003Cli>One syncs private DNS records to my UniFi router using a webhook provider\u003C/li>\n\u003Cli>The other pushes public records to Cloudflare\u003C/li>\n\u003C/ul>\n\u003Cp>Ingresses/HTTPRoutes are tagged with \u003Ccode>internal\u003C/code> or \u003Ccode>external\u003C/code> classes to control which DNS provider is used.\u003C/p>\n\u003Ch2 id=\"final-comments\">Final Comments\u003C/h2>\n\u003Cp>This setup has been a great learning experience and has allowed me to explore the Kubernetes ecosystem in depth. I plan on building and expanding my cluster with compute, deploying more complex platforms and services (looking at you Kubeflow) and building AI/ML workloads on Kubernetes!\u003C/p>",{"headings":25,"localImagePaths":39,"remoteImagePaths":40,"frontmatter":41,"imagePaths":44},[26,30,33,36],{"depth":27,"slug":28,"text":29},2,"the-cluster","The Cluster",{"depth":27,"slug":31,"text":32},"core-components","Core Components",{"depth":27,"slug":34,"text":35},"dns-with-split-horizon","DNS with Split Horizon",{"depth":27,"slug":37,"text":38},"final-comments","Final Comments",[],[],{"date":42,"title":14,"tags":43},"2025-02-28",[17,18],[],"bazel",{"id":45,"data":47,"body":51,"filePath":52,"digest":53,"rendered":54},{"title":48,"date":49,"tags":50},"Bazel Remote Build Execution",["Date","2025-06-17T00:00:00.000Z"],[17,18,45],"Bazel is pretty cool, it:\n\n- Is a build tool, with `rules` for language support e.g., [`rules_python`](https://github.com/bazel-contrib/rules_python)\n- All about that \"hermeticity\". How they get there is too big brain for me.\n- Made by Google after their internal Blaze build tool, so really made to go BRRR\n\nBecause of all this hermeticity stuff messages are ultra cryptic and who knows what they even mean. Tried using it at work but gave up because it's _pain_ to even decode what error messages are saying when you just want something to work and I was pretty much alone in using it.\n\nSeems like the kind of thing which is worth if an org has big investment in engineering infra and talent.\n\nI'm gonna give it another go. Might be more fun this time around and some rules have come a long way. For RBE platforms, there are not many self-hosted options but here's what I found:\n\n- [buildbarn/buildbarn](https://github.com/buildbarn/bb-deployments)\n  - Kinda nice but k8s skill issue didn't really know how to get it work a couple of years ago, I could probably figure it out now. Still not many examples to get the remote build part up and running. Written in go and configured in jsonnet.\n- [buildfarm/buildfarm](https://github.com/buildfarm/buildfarm)\n  - Has an OCI chart which would be easy to deploy but not many examples. Also hosting Java stuff kinda sucks because they love to eat RAM. Also not many examples. This is the original Bazel remote build but I guess they split off at some point.\n- [TraceMachina/nativelink](https://github.com/TraceMachina/nativelink)\n  - Think this used to be turbocache (a Rust-based CAS) but I guess they went and rebranded and now have a full RBE solution which is cool. Might try deploying this.\n\nThe play is to:\n\n1. Full send Claude Code on bashing its head against a monorepo until it figures out how to build things with Bazel\n2. Deploy a nativelink cluster, maybe fast-slow storage with memory as fast and Dragonfly/Redis as slow storage (lower) + NFS for slower storage (upper)\n3. ???\n4. Profit","src/content/blog/bazel.md","3b7c98d14f388c69",{"html":55,"metadata":56},"\u003Cp>Bazel is pretty cool, it:\u003C/p>\n\u003Cul>\n\u003Cli>Is a build tool, with \u003Ccode>rules\u003C/code> for language support e.g., \u003Ca href=\"https://github.com/bazel-contrib/rules_python\">\u003Ccode>rules_python\u003C/code>\u003C/a>\u003C/li>\n\u003Cli>All about that “hermeticity”. How they get there is too big brain for me.\u003C/li>\n\u003Cli>Made by Google after their internal Blaze build tool, so really made to go BRRR\u003C/li>\n\u003C/ul>\n\u003Cp>Because of all this hermeticity stuff messages are ultra cryptic and who knows what they even mean. Tried using it at work but gave up because it’s \u003Cem>pain\u003C/em> to even decode what error messages are saying when you just want something to work and I was pretty much alone in using it.\u003C/p>\n\u003Cp>Seems like the kind of thing which is worth if an org has big investment in engineering infra and talent.\u003C/p>\n\u003Cp>I’m gonna give it another go. Might be more fun this time around and some rules have come a long way. For RBE platforms, there are not many self-hosted options but here’s what I found:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://github.com/buildbarn/bb-deployments\">buildbarn/buildbarn\u003C/a>\n\u003Cul>\n\u003Cli>Kinda nice but k8s skill issue didn’t really know how to get it work a couple of years ago, I could probably figure it out now. Still not many examples to get the remote build part up and running. Written in go and configured in jsonnet.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\u003Ca href=\"https://github.com/buildfarm/buildfarm\">buildfarm/buildfarm\u003C/a>\n\u003Cul>\n\u003Cli>Has an OCI chart which would be easy to deploy but not many examples. Also hosting Java stuff kinda sucks because they love to eat RAM. Also not many examples. This is the original Bazel remote build but I guess they split off at some point.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\u003Ca href=\"https://github.com/TraceMachina/nativelink\">TraceMachina/nativelink\u003C/a>\n\u003Cul>\n\u003Cli>Think this used to be turbocache (a Rust-based CAS) but I guess they went and rebranded and now have a full RBE solution which is cool. Might try deploying this.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Cp>The play is to:\u003C/p>\n\u003Col>\n\u003Cli>Full send Claude Code on bashing its head against a monorepo until it figures out how to build things with Bazel\u003C/li>\n\u003Cli>Deploy a nativelink cluster, maybe fast-slow storage with memory as fast and Dragonfly/Redis as slow storage (lower) + NFS for slower storage (upper)\u003C/li>\n\u003Cli>???\u003C/li>\n\u003Cli>Profit\u003C/li>\n\u003C/ol>",{"headings":57,"localImagePaths":58,"remoteImagePaths":59,"frontmatter":60,"imagePaths":63},[],[],[],{"date":61,"title":48,"tags":62},"2025-06-17",[17,18,45],[],"deploying-kubeflow",{"id":64,"data":66,"body":71,"filePath":72,"digest":73,"rendered":74},{"title":67,"date":68,"tags":69},"Deploying Kubeflow",["Date","2025-05-06T00:00:00.000Z"],[70,17],"machine-learning","### Preamble\n\nI recently started contributing to the Kubeflow open-source project and have been following some recent GitHub discussions, watched Working Group (WG) call recordings (they run past midnight in my timezone unfortunately) and community calls. Many contributors come from organizations that deploy Kubeflow internally or offer it as a managed service. One of the more popular requests from the community is to build Helm charts for deploying the Kubeflow platform.\n\n### What on earth is Kubeflow?\n\nThe Kubeflow platform is a collection of machine learning tools designed to be deployed on Kubernetes. Kubeflow is deployed via a community-maintained repository, kubeflow/manifests, which provides a collection of Kustomize manifests for each component. These manifests include Istio configuration, pod security standards, network policies, and token-based authorization. The repository holds a collection of Kustomize manifests upstreamed from each Kubeflow component. It also implements an [Istio](https://istio.io) service mesh which secures traffic between components along with other security measures like pod security standards, network policies, and authorization tokens. It can also be configured to use an external OAuth2 provider. The scope of designing Kubeflow Helm charts is enormous and no easy effort, which makes sense why the Kubeflow organisation chooses not to maintain a central distribution of Kubeflow. Like many open-source projects, contributions often come from developers funded by their organizations or volunteers contributing in their free time. Kubeflow is already extremely complex to maintain to be security compliant for enterprises and has its own challenges designing and coordinating AI/ML tooling on Kubernetes. Vendors tend to provide enterprise support for deploying the platform.\n\nI recently went through deploying a customised Kubeflow. I operate a [bare-metal Kubernetes cluster](https://github.com/solanyn/home-ops) using GitOps powered by [Flux](https://fluxcd.io). A Flux controller in my cluster reads the latest changes in the repository and synchronises changes to my cluster. I usually reach for Helm charts since I find that they are the simplest to configure but occasionally deploy operators and/or Custom Resource Definitions (CRDs) from manifests if this is not an option. I recently moved away from [Ingress NGINX Controller](https://kubernetes.github.io/ingress-nginx/) to the new [Gateway API](https://gateway-api.sigs.k8s.io) using the [Cilium Gateway implementation](https://docs.cilium.io/en/stable/network/servicemesh/gateway-api/gateway-api/). This posed challenges because Istio's Gateway implementation and the Gateway API's evolving spec introduced compatibility and configuration issues. This should be resolved with the next Kubeflow platform release.\n\nDeploying the Kubeflow platform has a few external dependencies:\n\n- S3-compatible storage\n- MySQL/MySQL-compatible database\n- Istio service mesh\n- Cert Manager for certificates\n\nI already deploy Cert Manager for certificates for my split-brain DNS setup to publicly expose services via Cloudflare DNS and Cloudflare tunnels and internal (only on my network) using External DNS and the external-dns-unifi-webhook and a MinIO deployment with an NFS backend to my NAS. I settled on MariaDB Operator for its convenient CRDs to create databases, users and grants and scheduled S3 backup and restore procedures.\n\n### Kubeflow and Fluxtomizations\n\nI largely used the [deployKF](https://github.com/deployKF/deployKF) repository as inspiration which is a more configurable setup using gomplate, ArgoCD and raw bash scripts to configure settings. It's also a couple of revisions old (my guess is the maintainer started vendoring the distribution for later Kubeflow versions). It uses the same mechanism Helm values are templated into charts to create customised deployments AND kustomizations. I ended up opting for configurable options through Kubernetes Secret and ConfigMap objects. One of the biggest challenges was identifying all the points where MySQL and MinIO credentials were needed and using Kustomize patches to use a centralised secret. I should mention that secrets can be populated manually, using GitOps with [sops](https://github.com/getsops/sops) encryption (natively supported in Flux) or use [External Secrets Operator](https://external-secrets.io/latest/) to populate secrets from an external secret store. I personally use 1Password to populate my secrets which allows me to conveniently autofill credentials.\n\nRoughly speaking I went through the following steps:\n\n1. Write a GitHub Actions workflow to commit the latest non-release candidate tag of the manifests to my repository\n2. Add components to generate namespaces with the appropriate labels e.g., for Istio injection\n3. Configure secrets and OAuth2 settings for `dex` and `oauth2-proxy` using ExternalSecret and ConfigMap resources\n4. Add Kustomize patches to patch `dex` and `oauth2-proxy` deployments to mount the the configuration from secrets and/or configmaps\n5. Add an ExternalSecret manifest with configurations for ALL Kubeflow components\n6. Create Database objects for Kubeflow components that use MySQL/MySQL-compatible databases\n7. Add Kustomize patches to patch Kubeflow components (mainly Katib, Pipelines and Model Registry) to connect to my MariaDB cluster and the appropriate tables\n8. Add Kustomize patches to patch Kubeflow Pipelines to connect to my MinIO deployment\n9. Add any additional Istio RequestAuthentication manifests to enable authorization from GitHub and Pocket ID\n\n### Conclusion\n\nWhile future changes to the manifests could require rework, using Kustomize patches makes changes relatively easy to reconfigure. Being a contributor helps here, I would be aware of any breaking changes that could be introduced. Looking forward to the ongoing improvements like Gateway API support and Istio Ambient mode and how things will break in my cluster! Thanks for reading.","src/content/blog/deploying-kubeflow.md","dd73a11ad0e52fe7",{"html":75,"metadata":76},"\u003Ch3 id=\"preamble\">Preamble\u003C/h3>\n\u003Cp>I recently started contributing to the Kubeflow open-source project and have been following some recent GitHub discussions, watched Working Group (WG) call recordings (they run past midnight in my timezone unfortunately) and community calls. Many contributors come from organizations that deploy Kubeflow internally or offer it as a managed service. One of the more popular requests from the community is to build Helm charts for deploying the Kubeflow platform.\u003C/p>\n\u003Ch3 id=\"what-on-earth-is-kubeflow\">What on earth is Kubeflow?\u003C/h3>\n\u003Cp>The Kubeflow platform is a collection of machine learning tools designed to be deployed on Kubernetes. Kubeflow is deployed via a community-maintained repository, kubeflow/manifests, which provides a collection of Kustomize manifests for each component. These manifests include Istio configuration, pod security standards, network policies, and token-based authorization. The repository holds a collection of Kustomize manifests upstreamed from each Kubeflow component. It also implements an \u003Ca href=\"https://istio.io\">Istio\u003C/a> service mesh which secures traffic between components along with other security measures like pod security standards, network policies, and authorization tokens. It can also be configured to use an external OAuth2 provider. The scope of designing Kubeflow Helm charts is enormous and no easy effort, which makes sense why the Kubeflow organisation chooses not to maintain a central distribution of Kubeflow. Like many open-source projects, contributions often come from developers funded by their organizations or volunteers contributing in their free time. Kubeflow is already extremely complex to maintain to be security compliant for enterprises and has its own challenges designing and coordinating AI/ML tooling on Kubernetes. Vendors tend to provide enterprise support for deploying the platform.\u003C/p>\n\u003Cp>I recently went through deploying a customised Kubeflow. I operate a \u003Ca href=\"https://github.com/solanyn/home-ops\">bare-metal Kubernetes cluster\u003C/a> using GitOps powered by \u003Ca href=\"https://fluxcd.io\">Flux\u003C/a>. A Flux controller in my cluster reads the latest changes in the repository and synchronises changes to my cluster. I usually reach for Helm charts since I find that they are the simplest to configure but occasionally deploy operators and/or Custom Resource Definitions (CRDs) from manifests if this is not an option. I recently moved away from \u003Ca href=\"https://kubernetes.github.io/ingress-nginx/\">Ingress NGINX Controller\u003C/a> to the new \u003Ca href=\"https://gateway-api.sigs.k8s.io\">Gateway API\u003C/a> using the \u003Ca href=\"https://docs.cilium.io/en/stable/network/servicemesh/gateway-api/gateway-api/\">Cilium Gateway implementation\u003C/a>. This posed challenges because Istio’s Gateway implementation and the Gateway API’s evolving spec introduced compatibility and configuration issues. This should be resolved with the next Kubeflow platform release.\u003C/p>\n\u003Cp>Deploying the Kubeflow platform has a few external dependencies:\u003C/p>\n\u003Cul>\n\u003Cli>S3-compatible storage\u003C/li>\n\u003Cli>MySQL/MySQL-compatible database\u003C/li>\n\u003Cli>Istio service mesh\u003C/li>\n\u003Cli>Cert Manager for certificates\u003C/li>\n\u003C/ul>\n\u003Cp>I already deploy Cert Manager for certificates for my split-brain DNS setup to publicly expose services via Cloudflare DNS and Cloudflare tunnels and internal (only on my network) using External DNS and the external-dns-unifi-webhook and a MinIO deployment with an NFS backend to my NAS. I settled on MariaDB Operator for its convenient CRDs to create databases, users and grants and scheduled S3 backup and restore procedures.\u003C/p>\n\u003Ch3 id=\"kubeflow-and-fluxtomizations\">Kubeflow and Fluxtomizations\u003C/h3>\n\u003Cp>I largely used the \u003Ca href=\"https://github.com/deployKF/deployKF\">deployKF\u003C/a> repository as inspiration which is a more configurable setup using gomplate, ArgoCD and raw bash scripts to configure settings. It’s also a couple of revisions old (my guess is the maintainer started vendoring the distribution for later Kubeflow versions). It uses the same mechanism Helm values are templated into charts to create customised deployments AND kustomizations. I ended up opting for configurable options through Kubernetes Secret and ConfigMap objects. One of the biggest challenges was identifying all the points where MySQL and MinIO credentials were needed and using Kustomize patches to use a centralised secret. I should mention that secrets can be populated manually, using GitOps with \u003Ca href=\"https://github.com/getsops/sops\">sops\u003C/a> encryption (natively supported in Flux) or use \u003Ca href=\"https://external-secrets.io/latest/\">External Secrets Operator\u003C/a> to populate secrets from an external secret store. I personally use 1Password to populate my secrets which allows me to conveniently autofill credentials.\u003C/p>\n\u003Cp>Roughly speaking I went through the following steps:\u003C/p>\n\u003Col>\n\u003Cli>Write a GitHub Actions workflow to commit the latest non-release candidate tag of the manifests to my repository\u003C/li>\n\u003Cli>Add components to generate namespaces with the appropriate labels e.g., for Istio injection\u003C/li>\n\u003Cli>Configure secrets and OAuth2 settings for \u003Ccode>dex\u003C/code> and \u003Ccode>oauth2-proxy\u003C/code> using ExternalSecret and ConfigMap resources\u003C/li>\n\u003Cli>Add Kustomize patches to patch \u003Ccode>dex\u003C/code> and \u003Ccode>oauth2-proxy\u003C/code> deployments to mount the the configuration from secrets and/or configmaps\u003C/li>\n\u003Cli>Add an ExternalSecret manifest with configurations for ALL Kubeflow components\u003C/li>\n\u003Cli>Create Database objects for Kubeflow components that use MySQL/MySQL-compatible databases\u003C/li>\n\u003Cli>Add Kustomize patches to patch Kubeflow components (mainly Katib, Pipelines and Model Registry) to connect to my MariaDB cluster and the appropriate tables\u003C/li>\n\u003Cli>Add Kustomize patches to patch Kubeflow Pipelines to connect to my MinIO deployment\u003C/li>\n\u003Cli>Add any additional Istio RequestAuthentication manifests to enable authorization from GitHub and Pocket ID\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"conclusion\">Conclusion\u003C/h3>\n\u003Cp>While future changes to the manifests could require rework, using Kustomize patches makes changes relatively easy to reconfigure. Being a contributor helps here, I would be aware of any breaking changes that could be introduced. Looking forward to the ongoing improvements like Gateway API support and Istio Ambient mode and how things will break in my cluster! Thanks for reading.\u003C/p>",{"headings":77,"localImagePaths":91,"remoteImagePaths":92,"frontmatter":93,"imagePaths":96},[78,82,85,88],{"depth":79,"slug":80,"text":81},3,"preamble","Preamble",{"depth":79,"slug":83,"text":84},"what-on-earth-is-kubeflow","What on earth is Kubeflow?",{"depth":79,"slug":86,"text":87},"kubeflow-and-fluxtomizations","Kubeflow and Fluxtomizations",{"depth":79,"slug":89,"text":90},"conclusion","Conclusion",[],[],{"date":94,"title":67,"tags":95},"2025-05-06",[70,17],[]]